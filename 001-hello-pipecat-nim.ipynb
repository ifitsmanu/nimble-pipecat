{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60caf-a890-4e62-8255-62fd691cd6e6",
   "metadata": {},
   "source": [
    "# Voice Agent for Conversational AI with Pipecat\n",
    "In this notebook, we walk through how to craft and deploy a voice AI bot using Pipecat AI. We illustrate the basic Pipecat flow with the `meta/llama-3.3-70b-instruct`* LLM model and Riva for STT (Speech-To-Text) & TTS (Text-To-Speech). However, Pipecat is not opinionated and other models and STT/TTS services can easily be used. See [Pipecat documentation](https://docs.pipecat.ai/server/services/supported-services#supported-services) for other supported services.\n",
    "\n",
    "Pipecat AI is an open-source framework for building voice and multimodal conversational agents. Pipecat simplifies the complex voice-to-voice AI pipeline, and lets developers build AI capabilities easily and with Open Source, commercial, and custom models. See [Pipecat's Core Concepts](https://docs.pipecat.ai/getting-started/core-concepts) for a deep dive into how it works.\n",
    "\n",
    "The framework was developed by Daily, a company that has provided real-time video and audio communication infrastructure since 2016. It is fully vendor neutral and is not tightly coupled to Daily's infrastructure. That said, we do use it in this demo. Sign up for a Daily-bot API key [here](https://bots.daily.co/sign-up).\n",
    "\n",
    "> [Development Note]: *We are using \"meta/llama-3.3-70b-instruct\" for now because it works with tool calling, but can update/change this model at any time. It is a one line change in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa7d7-88fb-4b33-8145-ee1a91e58af1",
   "metadata": {},
   "source": [
    "## Step 1 - Install dependencies\n",
    "First we set our environment.\n",
    "\n",
    "We use Daily for transport, OpenAI for context aggregation, Riva for TTS & TTS, and Silero for VAD (Voice Activity Detection). If using different services, for example Cartesia for TTS, one would run `pip install pipecat-ai[cartesia]`.\n",
    "\n",
    "> [Development note]: We're installing from the github main branch here to ensure we have the latest improvements. By the time we address feedback we'll have a new release of Pipecat and just install the pipecat parts we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d7f76-bb78-4614-ab77-229ed3eea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "!pip install \"git+https://github.com/pipecat-ai/pipecat.git@main\"\n",
    "!pip install \"pipecat-ai[daily,openai,riva,silero]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c5d1-97a9-42e7-9de2-88b7d31b1409",
   "metadata": {},
   "source": [
    "## Step 2 - Configure Daily transport for WebRTC communication\n",
    "- room_url: Where to connect (and where will navigate to to talk to our bot)\n",
    "- None: No authentication token needed\n",
    "- \"NVIDIA NIM\": The bot's display name\n",
    "- Enable audio output for text-to-speech playback and enable VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb09c6-b9f7-409a-9ad4-742f494f6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to talk to the NVIDIA NIM bot\n",
    "# Update to your room url after obtaining Daily-bot API key\n",
    "#### NOTE: if this is changed, the link in Step 11 markdown will no longer work.\n",
    "DAILY_SAMPLE_ROOM_URL=\"https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1be9b-e052-4430-b7e1-d7bf57a5ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from pipecat.transports.services.daily import DailyParams, DailyTransport\n",
    "\n",
    "transport = DailyTransport(\n",
    "    DAILY_SAMPLE_ROOM_URL,\n",
    "    None,\n",
    "    \"NVIDIA NIM Agent\",\n",
    "    DailyParams(\n",
    "        audio_out_enabled=True,\n",
    "        vad_enabled=True,\n",
    "        vad_analyzer=SileroVADAnalyzer(),\n",
    "        vad_audio_passthrough=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506527e-b84c-49e1-8af4-223fdb33f582",
   "metadata": {},
   "source": [
    "## Step 3 - Initialize LLM, STT, and TTS services\n",
    "We can customize options, for example a different LLM `model` or `voice_id` for FastPitch TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d77d5-c183-43d0-980d-fd99a2836365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pipecat.services.nim import NimLLMService\n",
    "from pipecat.services.riva import FastPitchTTSService, ParakeetSTTService\n",
    "\n",
    "stt = ParakeetSTTService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "llm = NimLLMService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), model=\"meta/llama-3.3-70b-instruct\"\n",
    ")\n",
    "\n",
    "tts = FastPitchTTSService(api_key=os.getenv(\"NVIDIA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac150732-cbb4-4c70-8d31-cab5ae51b5fb",
   "metadata": {},
   "source": [
    "## Step 4 - Define LLM prompt\n",
    "Edit the prompt as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d884775-c4c0-49eb-b502-d4c855cc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way and make a weather pun if it is possible.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923291b-4823-46f9-88f6-e88b42bf3191",
   "metadata": {},
   "source": [
    "## Step 5 - Define tool calling function\n",
    "Here we use the classic \"get_weather\" example. We use OpenAI's ChatCompletionToolParam and register the function with the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e13ce0-5077-4ccc-be54-12932d816542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionToolParam\n",
    "from pipecat.frames.frames import TextFrame\n",
    "\n",
    "\n",
    "async def start_fetch_weather(function_name, llm, context):\n",
    "    await llm.push_frame(TextFrame(\"Let me check on that.\"))\n",
    "    print(f\"Starting fetch_weather_from_api with function_name: {function_name}\")\n",
    "\n",
    "async def fetch_weather_from_api(function_name, tool_call_id, args, llm, context, result_callback):\n",
    "    await result_callback({\"conditions\": \"nice\", \"temperature\": \"75\"})\n",
    "\n",
    "tools = [\n",
    "            ChatCompletionToolParam(\n",
    "                type=\"function\",\n",
    "                function={\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Returns the current weather at a location, if one is specified, and defaults to the user's location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to find the weather of, or if not provided, it's the default location.\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"Whether to use SI or USCS units (celsius or fahrenheit).\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "\n",
    "llm.register_function(None, fetch_weather_from_api, start_callback=start_fetch_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fc5c4-e667-4ba7-bce4-de397cc40000",
   "metadata": {},
   "source": [
    "## Step 6 - Initialize the Context Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576f72b-556e-4218-92e3-a06bbd5bf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "context = OpenAILLMContext(messages, tools)\n",
    "context_aggregator = llm.create_context_aggregator(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752c614-a65d-4c61-965f-26d7b46f8153",
   "metadata": {},
   "source": [
    "## Step 7 - Create pipeline\n",
    "Here we align the services into a pipeline to process speech into text, send to llm, then turn the llm response text into speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8620a2-4caa-40c5-88d9-8aca2743157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),              # Transport user input\n",
    "        stt,                            # STT\n",
    "        context_aggregator.user(),      # User responses\n",
    "        llm,                            # LLM\n",
    "        tts,                            # TTS\n",
    "        transport.output(),             # Transport bot output\n",
    "        context_aggregator.assistant(), # Assistant spoken responses\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c588f-0c00-4414-984a-33da31e2803d",
   "metadata": {},
   "source": [
    "## Step 8 - Create PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbadb9a-9778-4f0f-910f-5c53d117e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "\n",
    "task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4890ce7-6a1a-4f39-b6af-9a3335ad9fcf",
   "metadata": {},
   "source": [
    "## Step 9 - Create a pipeline runner\n",
    "This manages the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e504ab-b889-4b6a-96a1-159d42a95833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "\n",
    "runner = PipelineRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162c265-39cd-49d1-beb6-fcf368572156",
   "metadata": {},
   "source": [
    "## Step 10 - Set event handlers\n",
    "The `on_first_participant_joined` handler tells the bot to start the conversation when you join the call.  \n",
    "The `on_participant_left` handler sends an EndFrame which signals to terminate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917234-efc6-440d-b427-ca4acab0b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.frames.frames import LLMMessagesFrame, EndFrame\n",
    "\n",
    "@transport.event_handler(\"on_first_participant_joined\")\n",
    "async def on_first_participant_joined(transport, participant):\n",
    "    # Kick off the conversation.\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Please introduce yourself to the user and deliver a weather fact.\"})\n",
    "    await task.queue_frames([LLMMessagesFrame(messages)])\n",
    "\n",
    "@transport.event_handler(\"on_participant_left\")\n",
    "async def on_participant_left(transport, participant, reason):\n",
    "    print(f\"Participant left: {participant}\")\n",
    "    await task.queue_frame(EndFrame())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998f8d-ac33-4b38-b10a-01691f81636a",
   "metadata": {},
   "source": [
    "## Step 11 - Run the Agent!\n",
    "\n",
    "Once you have run the code block below, you can talk to the agent at\n",
    "#### [https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test](https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test) \n",
    "to open a new browser window connected to the agent's WebRTC session.\n",
    "\n",
    "### Suggested conversations:\n",
    "- *Try tool calling.* As the bot about the weather.\n",
    "- *Observe the agent's context \"memory\".* Ask the agent to recite the alphabet. Interrupt the agent in the middle of the alphabet and ask it another question (What is 256 times 3?). Ask it a few more things... then ask it to pick up where it left off in reciting the alphabet.\n",
    "- *Play.* Ask the agent to tell a joke. Interrupt it. Ask it to explain the pythagorean theorem.\n",
    "\n",
    "The first time you run the bot, it will load weights for a voice activity model into the local Python process. This will take 10-15 seconds. A permissions dialog will ask you to allow the browser to access your camera and microphone. Click yes to start talking to the bot. If you have any trouble with this, see [here](https://help.daily.co/en/articles/2525908-allow-camera-and-mic-access).\n",
    "\n",
    "> [Development Note]: We can build the microphone input and speaker output into the notebook so that it isn't necessary to open another browser window. We will put this on our todo list, along with any feedback we get on Friday from the initial testing of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a411cb-d2c8-4446-be69-b391486e853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runner.run(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
